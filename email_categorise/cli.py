from __future__ import annotations

import argparse
import logging
import subprocess
import uuid
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import List, Optional

from .config import load_config, AccountConfig
from .fine_tune import export_reply_dataset, train_local_reply_model
from .model_client import StructuredLLMRunner, build_model_clients
from .triage_logic import (
    init_account,
    run_for_account,
    rollback_run,
    send_failure_notification,
)
from .utils import configure_logging, load_env_file, utc_now, load_json, save_json

logger = logging.getLogger("email_categorise.cli")


def _maybe_run_tests(cfg, accounts: List) -> None:
    """
    If tests haven't run in the last 7 days, run pytest.
    On failure: send notification and exit.
    """
    data_dir = Path(cfg.repo_root) / "data"
    data_dir.mkdir(parents=True, exist_ok=True)
    marker = data_dir / "last_tests.json"
    now = datetime.now(timezone.utc)
    meta = load_json(marker, {})
    last_run_str = meta.get("last_run")
    if last_run_str:
        try:
            last_run = datetime.fromisoformat(last_run_str)
            if now - last_run < timedelta(days=7):
                return
        except Exception:
            pass

    logger.info("Weekly test guard: running pytest...")
    result = subprocess.run(["pytest"], cwd=cfg.repo_root)
    if result.returncode == 0:
        save_json(marker, {"last_run": now.isoformat()})
        return

    logger.error("Pytest failed (exit %s); stopping execution.", result.returncode)
    sent = send_failure_notification(
        cfg,
        accounts,
        subject="Email categorise tests failing - run aborted",
        body_html=(
            "<p>Automated weekly tests failed. "
            "Triage run was aborted. Please check the logs and fix tests.</p>"
        ),
        run_id="tests",
    )
    if not sent:
        logger.error("Could not send failure notification email.")
    raise SystemExit("Tests failed; aborting run.")


def _select_accounts(
    accounts: List[AccountConfig], selected: Optional[List[str]]
) -> List[AccountConfig]:
    if not selected:
        return accounts
    s = {x.lower() for x in selected}
    out = [a for a in accounts if a.email.lower() in s]
    if not out:
        raise SystemExit("No accounts matched -a filters")
    return out


def _write_report(repo_root: Path, name: str, content: str) -> Path:
    out_dir = repo_root / "output"
    out_dir.mkdir(parents=True, exist_ok=True)
    stamp = utc_now().strftime("%Y%m%d-%H%M%S")
    path = out_dir / f"email_categorise_{name}_{stamp}.last.md"
    path.write_text(content, encoding="utf-8")
    return path


def main() -> None:
    load_env_file()
    parser = argparse.ArgumentParser("email_categorise")
    sub = parser.add_subparsers(dest="cmd", required=True)

    default_config = Path(__file__).resolve().parent.parent / "config" / "config.toml"

    p_init = sub.add_parser("init")
    p_init.add_argument(
        "--config",
        "-c",
        default=str(default_config),
        help=f"Path to config TOML (default: {default_config})",
    )
    p_init.add_argument("-a", "--account", action="append")
    p_init.add_argument("-v", "--verbose", action="count", default=0)
    p_init.add_argument("--run-id", help="Optional run id for this init session")

    p_run = sub.add_parser("run")
    p_run.add_argument(
        "--config",
        "-c",
        default=str(default_config),
        help=f"Path to config TOML (default: {default_config})",
    )
    p_run.add_argument("-a", "--account", action="append")
    p_run.add_argument("-v", "--verbose", action="count", default=0)
    p_run.add_argument(
        "--draft-replies",
        action=argparse.BooleanOptionalAction,
        dest="draft_replies",
        default=None,
        help="Enable/disable drafting replies for this run",
    )
    p_run.add_argument(
        "--create-tasks",
        action=argparse.BooleanOptionalAction,
        dest="create_tasks",
        default=None,
        help="Enable/disable task creation for this run",
    )
    p_run.add_argument(
        "--summary-email",
        action=argparse.BooleanOptionalAction,
        dest="summary_email",
        default=None,
        help="Enable/disable sending summary email for this run",
    )
    p_run.add_argument(
        "--log-to-file",
        action=argparse.BooleanOptionalAction,
        dest="log_to_file",
        default=None,
        help="Enable/disable writing triage-log.txt for this run",
    )
    p_run.add_argument(
        "--run-id",
        help="Optional run id; autogenerated if omitted (used for ledger/rollback)",
    )
    p_run.add_argument(
        "--undo-last",
        action="store_true",
        help="Rollback the most recent run recorded in the ledger",
    )
    p_run.add_argument(
        "--rollback", help="Rollback a specific run-id recorded in the ledger"
    )

    p_ft = sub.add_parser("export-finetune")
    p_ft.add_argument(
        "--config",
        "-c",
        default=str(default_config),
        help=f"Path to config TOML (default: {default_config})",
    )
    p_ft.add_argument("-a", "--account", action="append")
    p_ft.add_argument("-v", "--verbose", action="count", default=0)
    p_ft.add_argument(
        "--output-dir",
        default="output/finetune",
        help="Directory to write fine-tune JSONL files (default: output/finetune)",
    )
    p_ft.add_argument(
        "--max-messages",
        type=int,
        default=500,
        help="Maximum Sent Items messages to export per account (default: 500)",
    )

    p_ft_train = sub.add_parser("train-finetune")
    p_ft_train.add_argument(
        "--dataset",
        required=True,
        help="Path to JSONL dataset exported with 'export-finetune'",
    )
    p_ft_train.add_argument(
        "--base-model",
        required=True,
        help="Base Hugging Face model id to fine-tune (e.g. meta-llama/Meta-Llama-3.1-8B-Instruct)",
    )
    p_ft_train.add_argument(
        "--output-dir",
        required=True,
        help="Directory to save the fine-tuned model (HF local path)",
    )
    p_ft_train.add_argument(
        "--max-steps",
        type=int,
        default=500,
        help="Maximum training steps (default: 500)",
    )
    p_ft_train.add_argument(
        "--learning-rate",
        type=float,
        default=5e-5,
        help="Learning rate for fine-tuning (default: 5e-5)",
    )
    p_ft_train.add_argument(
        "--batch-size",
        type=int,
        default=1,
        help="Per-device train batch size (default: 1)",
    )
    p_ft_train.add_argument(
        "--max-seq-len",
        type=int,
        default=1024,
        help="Maximum sequence length for training (default: 1024)",
    )

    args = parser.parse_args()
    configure_logging(args.verbose or 0)

    cfg = load_config(args.config)

    # Per-run overrides (do not persist)
    if args.cmd == "run":
        if args.draft_replies is not None:
            cfg.triage.draft_replies = bool(args.draft_replies)
        if args.create_tasks is not None:
            cfg.triage.create_tasks = bool(args.create_tasks)
        if args.summary_email is not None:
            cfg.triage.send_summary_email = bool(args.summary_email)
        if args.log_to_file is not None:
            cfg.triage.log_to_file = bool(args.log_to_file)

    accounts = _select_accounts(cfg.accounts, args.account)

    runner_triage: Optional[StructuredLLMRunner] = None
    runner_reply: Optional[StructuredLLMRunner] = None
    if args.cmd in {"run", "init"}:
        # Build model clients/runners for triage + reply using the model registry.
        model_clients = build_model_clients(cfg)
        triage_key = cfg.llm.triage_model
        reply_key = cfg.llm.reply_model
        if triage_key not in model_clients:
            raise SystemExit(
                f"Unknown triage model '{triage_key}' in config; available models: "
                f"{', '.join(sorted(model_clients.keys())) or '(none)'}"
            )
        if reply_key not in model_clients:
            raise SystemExit(
                f"Unknown reply model '{reply_key}' in config; available models: "
                f"{', '.join(sorted(model_clients.keys())) or '(none)'}"
            )

        runner_triage = StructuredLLMRunner(client=model_clients[triage_key])
        runner_reply = StructuredLLMRunner(client=model_clients[reply_key])

    # Weekly test guard
    if args.cmd in {"run", "init"}:
        _maybe_run_tests(cfg, accounts)

    if args.cmd == "run" and (args.undo_last or args.rollback):
        ledger_dir = cfg.repo_root / "ledger"
        index_path = ledger_dir / "index.json"
        target_run = args.rollback
        if args.undo_last:
            index = load_json(index_path, {"order": []})
            order = index.get("order") or []
            if not order:
                raise SystemExit("No runs recorded in ledger to undo.")
            target_run = order[-1]
        if not target_run:
            raise SystemExit("No run-id provided for rollback.")
        res = rollback_run(cfg, accounts, target_run)
        print(f"Rollback complete for run {target_run}: {res}")
        return

    if args.cmd == "init":
        rows = []
        run_id = args.run_id or uuid.uuid4().hex[:12]
        for a in accounts:
            logger.info("Initialising %s (%s)", a.email, a.label)
            res = init_account(cfg, a, runner_reply, run_id=run_id)
            rows.append(res)
        md = (
            "# init report\n\n"
            + "\n".join(
                [
                    f"- **{r['account']}**: sender_stats={r['sender_stats']}, tone_contacts={r['tone_contacts']}"
                    for r in rows
                ]
            )
            + "\n"
        )
        report_path = _write_report(cfg.repo_root, "init", md)
        print(f"Report: {report_path}")
        return

    if args.cmd == "run":
        rows = []
        run_id = args.run_id or uuid.uuid4().hex[:12]
        for a in accounts:
            logger.info("Running triage for %s (%s)", a.email, a.label)
            res = run_for_account(cfg, a, runner_triage, runner_reply, run_id=run_id)
            rows.append(res)
        md_lines = ["# run report", ""]
        for r in rows:
            md_lines.append(
                f"- **{r['account']}**: processed={r.get('processed')}, drafts={r.get('drafts')}, tasks={r.get('tasks')}, informational={r.get('informational')}, summary_sent={r.get('summary_sent')}"
            )
        md_lines.append("")
        report_path = _write_report(cfg.repo_root, "run", "\n".join(md_lines))
        print(f"Report: {report_path}")
        return

    if args.cmd == "export-finetune":
        out_dir = Path(cfg.repo_root) / args.output_dir
        out_dir.mkdir(parents=True, exist_ok=True)
        rows = []
        for a in accounts:
            logger.info("Exporting fine-tune dataset for %s (%s)", a.email, a.label)
            path = out_dir / f"finetune_{a.email.replace('@','_at_')}.jsonl"
            res = export_reply_dataset(cfg, a, path, max_messages=args.max_messages)
            rows.append(res)
        md_lines = ["# export-finetune report", ""]
        for r in rows:
            md_lines.append(
                f"- **{r['account']}**: examples={r['examples']}, path={r['path']}"
            )
        md_lines.append("")
        report_path = _write_report(cfg.repo_root, "export-finetune", "\n".join(md_lines))
        print(f"Report: {report_path}")
        return

    if args.cmd == "train-finetune":
        from pathlib import Path as _Path

        dataset_path = _Path(args.dataset)
        output_dir = _Path(args.output_dir)
        res = train_local_reply_model(
            dataset_path=dataset_path,
            base_model_id=args.base_model,
            output_dir=output_dir,
            max_steps=args.max_steps,
            learning_rate=args.learning_rate,
            batch_size=args.batch_size,
            max_seq_len=args.max_seq_len,
        )
        md_lines = [
            "# train-finetune report",
            "",
            f"- base_model: `{res['base_model']}`",
            f"- dataset: `{res['dataset']}`",
            f"- output_dir: `{res['output_dir']}`",
            f"- max_steps: {res['max_steps']}",
            f"- learning_rate: {res['learning_rate']}",
            f"- batch_size: {res['batch_size']}",
            f"- max_seq_len: {res['max_seq_len']}",
            "",
        ]
        report_path = _write_report(cfg.repo_root, "train-finetune", "\n".join(md_lines))
        print(f"Report: {report_path}")
        return
